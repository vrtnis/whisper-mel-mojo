# Whisperâ€‘Melâ€‘Mojo

A fast, portable **Mojo** kernel that fuses **logâ€‘Mel spectrogram** extraction and a **3â€¯Ã—â€¯3 average convolution**, ready to plug into **MAX Graph** or PyTorch as a custom op.

---

## Melâ€¯Spectrogramâ€¯and Whisperâ€¯Frontâ€‘End

A **Mel spectrogram** is a timeâ€“frequency representation of audio where:

- The **frequency axis is warped to the Mel scale**, matching human pitch perception by placing denser filterâ€‘banks at low frequencies and sparser ones at high frequencies.  
- **Amplitudes are mapped to decibels (log scale)**, reflecting the logarithmic way humans perceive loudness and compressing the very wide dynamic range of raw power values .  
- A compact 80â€‘bin logâ€‘Mel frame is the *deâ€‘facto* input feature for modern speech models, including Whisper and many Huggingâ€¯Face audio checkpoints .

---

### Whisper

[OpenAIâ€¯Whisper](https://openai.com/research/whisper) is an encoderâ€“decoder Transformer trained on 680â€¯kâ€¯h of multilingual speech.  Its frontâ€‘end expects:

| Requirement | Value |
|-------------|-------|
| Audio sample rate | **16â€¯kHz** |
| Spectrogram channels | **80 logâ€‘Mel bins** |
| FFT window / hop | **25â€¯ms / 10â€¯ms** |
| Chunk length | **30â€¯s** |




This project reâ€‘implement the **exact Whisper frontâ€‘end**â€”including the 3â€¯Ã—â€¯3 smoothing convolution in a single, hardwareâ€‘agnostic Mojo kernel, allowing the entire pipeline to stay onâ€‘device with zero hostâ†”device copies.



            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ WAV / PCM   â”‚ 16â€‘kHz mono
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Mojo logâ€‘Mel kernel   â”‚ 80Ã—T
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚  fused 3Ã—3 avgâ€‘pool
           â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Whisperâ€‘ready feature tensor â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
(MAX Graph / PyTorch op)


## ğŸ¯â€¯Features

- **Pureâ€¯Mojo, one file** â€“ the same source compiles for CPU, NVIDIA CUDA, Apple Metal, and (soon) AMDâ€¯ROCm via MAXâ€™s MLIR backâ€‘end :contentReference[oaicite:7]{index=7}.  
- **Dropâ€‘in MAXÂ Graph & PyTorch op** â€“ paste the kernel into `ops.custom` or expose it through `torch.ops` with no code changes; community examples already demonstrate the pattern :contentReference[oaicite:8]{index=8}.  
- **Zeroâ€‘copy execution** â€“ audio and feature buffers remain in unified GPU memory, avoiding redundant PCIe traffic and reducing peak host RAM :contentReference[oaicite:9]{index=9}.

---

## ğŸ›  Buildâ€¯&â€¯Run

```bash
# 1. Build the shared library
mojo build mel_pipeline_gpu.mojo --emit shared-lib -o libmel.so

# 2. Run the Python driver (benchmarks + sanity check)
python pipeline.py

